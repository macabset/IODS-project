2+2
#Bringing data to R.markdown + summaries
setwd(C:Users/murmeli/Documents/pappamurmeli/2013-2017/Säästöön/Hum.kand.2013-2017/Sivuaineet/Tilastotiede/Opendatascience/GitHub/IODS-final)
#Bringing data to R.markdown + summaries
setwd(C:users/murmeli/Documents/pappamurmeli/2013-2017/Säästöön/Hum.kand.2013-2017/Sivuaineet/Tilastotiede/Opendatascience/GitHub/IODS-final)
#Bringing data to R.markdown + summaries
setwd(C:murmeli/Documents/pappamurmeli/2013-2017/Säästöön/Hum.kand.2013-2017/Sivuaineet/Tilastotiede/Opendatascience/GitHub/IODS-final)
#Bringing data to R.markdown + summaries
setwd(C:Documents/pappamurmeli/2013-2017/Säästöön/Hum.kand.2013-2017/Sivuaineet/Tilastotiede/Opendatascience/GitHub/IODS-final)
?setwd()
#Bringing data to R.markdown + summaries
setwd("C:/Users/murmeli/Documents/pappamurmeli/2013-2017/Säästöön/Hum.kand.2013-2017/Sivuaineet/Tilastotiede/Opendatascience/GitHub/IODS-final")
getwd()
human2 <- read.csv("data/human2.csv", sep=",", header = TRUE, row.names = 1)
str(human2)
dim(human2)
head(human2)
#Complete cases, whithout missing values
complete.cases(human2)
#Packages we may need in this workout
library(GGally)
install.packages("GGally")
install.packages("dplyr","corrplot","FactoMiner","ggplot2","tidyr","MASS")
install.packages("dplyr")
install.packages("corrplot")
install.packages("FactoMiner")
install.packages("FactoMineR")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("tidyr")
install.packages("MASS")
#Packages we may need in this workout
library(GGally)
library(dplyr)
library(corrplot)
library(FactoMineR)
library(ggplot2)
library(tidyr)
library(MASS)
#Bringing data to R.markdown + summaries
setwd("C:/Users/murmeli/Documents/pappamurmeli/2013-2017/Säästöön/Hum.kand.2013-2017/Sivuaineet/Tilastotiede/Opendatascience/GitHub/IODS-final")
getwd()
human2 <- read.csv("data/human2.csv", sep=",", header = TRUE, row.names = 1)
str(human2)
dim(human2)
head(human2)
#Complete cases, whithout missing values
complete.cases(human2)
#Packages we may need in this workout
library(GGally)
library(dplyr)
library(corrplot)
library(FactoMineR)
library(ggplot2)
library(tidyr)
library(MASS)
cor_human<-cor(human2) %>% round(digits=2)
cor_human
#Visualize correlation matrix with a plot
title <- "Correlations between variables in human data"
corrplot(cor_human, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6, title = title,  mar=c(0,0,1,0))
summary(human2)
#Draw a density plot of each variable
gather(human2) %>% ggplot(aes(value,fill="#99999",alpha=0.3)) + facet_wrap("key", scales = "free") +
geom_density() +
ggtitle("Density plots of the variables in the human data") +
theme(plot.title = element_text(hjust = 0.5,size=16,face='bold'),legend.position="none")
# center and standardize variables
human_scaled <- scale(human2)
# summaries of the scaled variables
summary(human_scaled)
# class of the human_scaled object
class(human_scaled)
#change the object to data frame
human_scaled <- as.data.frame(human_scaled)
class(human_scaled)
# save the Edu.exp as scaled_eduexp
scaled_eduexp <- human_scaled$Edu.exp
# summary of the scaled_eduexp
summary(scaled_eduexp)
# create a quantile vector of eduexp and print it
bins <- quantile(scaled_eduexp)
bins
# create a new categorical variable 'eduexp'
eduexp <- cut(scaled_eduexp, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor
table(eduexp)
# remove original Edu.exp from the dataset
human_scaled <- dplyr::select(human_scaled, -Edu.exp)
# add the new categorical value to scaled data
human_scaled <- data.frame(human_scaled, eduexp)
str(human_scaled)
# number of rows in the human dataset
n <- nrow(human_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- human_scaled[ind,]
# create test set
test <- human_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$eduexp
summary(correct_classes)
# remove the eduexp variable from test data
test <- dplyr::select(test, -eduexp)
lda.fit <- lda(formula= eduexp ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$eduexp)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes, main = "Figure 3. LDA model 1, classes")
lda.arrows(lda.fit, myscale = 1)
#another plot with arrows showing clearly
plot(lda.fit, dimen=2, cex = c(0.1), main = "Figure 4. LDA model 1, arrows")
lda.arrows(lda.fit, myscale = 2)
human_scaled2 <-human_scaled
human_scaled2 <- dplyr::select(human_scaled2, -HDI)
# number of rows in the human dataset
n2 <- nrow(human_scaled2)
# choose randomly 80% of the rows
ind2 <- sample(n2,  size = n * 0.8)
# create train set
train2 <- human_scaled2[ind2,]
# create test set
test2 <- human_scaled2[-ind2,]
# save the correct classes from test data
correct_classes2 <- test2$eduexp
summary(correct_classes2)
# remove the eduexp variable from test data
test2 <- dplyr::select(test2, -eduexp)
lda.fit2 <- lda(formula= eduexp ~ ., data = train2)
# print the lda.fit object
lda.fit2
# the function for lda biplot arrows
lda.arrows2 <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes2 <- as.numeric(train2$eduexp)
# plot the lda results
plot(lda.fit2, dimen = 2, col=classes2, pch=classes2, main = "Figure 5. LDA model 2, classes")
lda.arrows2(lda.fit2, myscale = 1)
#another plot with arrows showing
plot(lda.fit2, dimen=2, cex = c(0.1), main = "Figure 6. LDA model 2, arrows")
lda.arrows2(lda.fit2, myscale = 2)
model_predictors <- dplyr::select(train2, -eduexp)
# check the dimensions
dim(model_predictors)
dim(lda.fit2$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit2$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
install.packages("plotly")
model_predictors <- dplyr::select(train2, -eduexp)
# check the dimensions
dim(model_predictors)
dim(lda.fit2$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit2$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train2$eduexp)
install.packages("knitr")
model_predictors <- dplyr::select(train2, -eduexp)
# check the dimensions
dim(model_predictors)
dim(lda.fit2$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit2$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train2$eduexp)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table()%>%addmargins()
# predict classes with test data
lda.pred2 <- predict(lda.fit2, newdata = test2)
# cross tabulate the results
table(correct = correct_classes2, predicted = lda.pred2$class)
table(correct = correct_classes2, predicted = lda.pred2$class) %>% prop.table()%>%addmargins()
human_scaled3 <- scale(human2)
human_scaled3 <- as.data.frame(human_scaled3)
human_scaled3 <- dplyr::select(human_scaled3, -HDI)
summary(human_scaled3)
class(human_scaled3)
human_scaled3<- as.data.frame(human_scaled3)
dist_eu <- dist(human_scaled3)
# look at the summary of the distances
summary(dist_eu)
library(ggplot2); library(GGally)
set.seed(123)
# euclidean distance matrix
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
# visualize the results
plot(1:k_max, twcss, type='b', main="Results of the total whithin sum of squares")
# k-means clustering
km <-kmeans(dist_eu, centers = 2)
human_scaled3$km <- as.factor(km$cluster)
# plot the human dataset with clusters
ggpairs(human_scaled3,  ggplot2::aes(colour=km), title="Paired variables with clusters")
# k-means again
set.seed(123)
km2 <- kmeans(dist_eu, centers = 4)
# LDA with using the k-means clusters as target classes
human_scaled3$cl <- km2$cluster
lda.fit3 <- lda(cl ~ ., data = human_scaled3)
lda.fit3
plot(lda.fit3, col=as.numeric(human_scaled3$cl), dimen=2, main = "Figure 9. LDA model 3")
lda.arrows(lda.fit3, myscale = 2, col = "#666666")
??"github"
?"git"
??git
setwd("C:/Users/murmeli/Documents/murmeli/2017/GitHub/IODS-project/data/")
getwd()
full_learn14<- read.table( "http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
str(full_learn14)
dim(full_learn14)
library(dplyr)
full_learn14$gender
#RStudio exercise 2, take 2
#Maija Absetz//27.1.2017 -> Modified in 6.11.2017 as assistant mode
#In this exercise we have been provided with a dataset from the course "Johdatus yhteiskuntatilastotieteeseen, syksy 2014".
#1. Data Wrangling (max 5p)
#sET THE WOTKING DIRECTORY TO MAKE DATA MORE OPEN -> modified wd, since I have a new computer -> new path
setwd("C:/Users/murmeli/Documents/murmeli/2017/GitHub/IODS-project/data/")
getwd()
#1.1 Getting our data to R and exploring its content:
full_learn14<- read.table( "http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
str(full_learn14)
dim(full_learn14)
#Our data has got 60 variables and 184 observations in those 60 variables.
#The structure shows what kind of observations do we have in our variables: factors, numbers, levels.
library(dplyr)
#1.2 Exploring and creating altogether 7 variables: gender, age, attitude, deep, stra, surf, points.
full_learn14$gender
full_learn14$Age
full_learn14$Points
#Create stra, deep and surf
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D07","D14","D22","D30")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
#Combine questions with mean and rename
deep_columns <- select(full_learn14, one_of(deep_questions))
full_learn14$deep <- rowMeans(deep_columns)
surface_columns <- select(full_learn14, one_of(surface_questions))
full_learn14$surf <- rowMeans(surface_columns)
strategic_columns <- select(full_learn14, one_of(strategic_questions))
full_learn14$stra <- rowMeans(strategic_columns)
#Pick columns for your sub_data
New_columns <- c("gender","Age","Attitude", "deep", "stra", "surf", "Points")
sub_learn14 <- select(full_learn14, one_of(New_columns))
#Rename all to small letters
colnames(sub_learn14)[2] <- "age"
colnames(sub_learn14)[3] <- "attitude"
colnames(sub_learn14)[7]<- "points"
colnames(sub_learn14)
str(sub_learn14)
#Exclude "0" values out of "points"
sub_learn14 <- filter(sub_learn14, points >! 0)
#1.3 Save to iods-project as csv format
?write.csv
write.csv(sub_learn14, file = "learning2014.csv", row.names = FALSE)
lrn2014 <- read.csv("learning2014.csv")
head(lrn2014)
#Part 2: Analysis (max 15p)
#2.1: Exploring our dataset
str(lrn2014)
dim(lrn2014)
# Here you can see the learning2014 dataset, whih has 7 variables with 166 observations.
#This questionnary conserning students's approaches to learning has been collected from from course:" Introduction to Social Statistics, fall 2014" (Finnish)
#Dataset has been modified so that questions have been combined and point values of 0 have been removed.
summary(lrn2014)
library(ggplot2)
#In our summary we can see that all variables except gender are numeric with continuous values.
#2.2 Picturing our dataset
A <- ggplot(lrn2014, aes(x= attitude, y=points, col=gender))
B <- A +geom_point()
C <- B + geom_smooth(method = "lm")
Figure1 <-C + ggtitle("The relationship between attitude and points")
Figure1
# Figure 1 shows us how attitude towards statistics correlate to the points for the exam. In this case, such in many others, motivation explains partly success.
#EXTRA: Combinig strategic learning to points.
D <- ggplot(lrn2014, aes(x=stra, y=points, col=gender))
D1 <- D + geom_point()
D2 <- D1 + geom_smooth(method="lm")
Figure2 <- D2 + ggtitle("The relationship between strategic learning and points")
Figure2
#If we look at Figure 2 there seems to be very small linear relationship between strategic learning and exam points. If student used strategic learning method in general, he or she might have gained a bit better exam points.
#2.3 Regression model
#Choosing explaining variables
library(GGally)
ggpairs(lrn2014, lower= list(combo = wrap("facethist", bins = 20)))
#As we can see in our plot matrix, variables attitude, stra and surf correlate the most with points. Lets try those as explanatory values in our regression model.
#Explaining points with attitude,strategic and surface learning
my_reg <- lm(points ~ attitude + stra + surf, data = lrn2014 )
summary(my_reg)
#Attitude seems to be the only one statistically significant in explaining points. This result is compatitive with our result in Figure 2, where correlation between strategic learning and points was bearly noticeable.
#Let's remove first surf and see again our model
my_reg2 <- lm(points ~ attitude + stra, data = lrn2014)
summary(my_reg2)
#The strategic learning gives still too high p-value, so we cannot exclude it being significant by chance.
#Let's try our regression model with only attitude explaining it.
my_reg3 <- lm(points ~ attitude, data=lrn2014)
summary(my_reg3)
#It seems that attitude towards statistics is our key explanatory variable to exam points. The more positive attitude toward statistics in general the better outcome in exam.
# 2.4 Interpreting R-squared
#Then we should look at our R-squared values to see how well our model fits to our observations.
#Our 3 tests of regression model show that our Multiple R-squared lowers each time we drop a variable. That happens, because it increases every time you add a variable.
#What we also see is that the 2nd test has the highest adjusted R-square, even though it has more variables than the last one.
# I conclude that we still should consider bringing back strategic learning to our multiple regression model, even though its p-value is over magical line of 0.05.
#2.5 Errors in our model
#Let's check Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.
?plot.lm
errors <- plot(my_reg2, which= c(1,2,5), par(mfrow= c(2,2)))
#In plot 3 we see, that no single observation has too much weight on the model.
#To conclude: our regression model 2, with explanatory variables attitude and strategic learning, exam points are explained with linear model. Considering the r-squared and checking with residuals, it indeed seems that the 2nd regression model best explains points. That means the more positive attitude and the more strategically you wish to learn statistics, the better outcome you get from statistics exam.
getwd()
lrn2014 <- read.csv("learning2014.csv")
head(lrn2014)
str(lrn2014)
dim(lrn2014)
summary(lrn2014)
library(ggplot2)
A <- ggplot(lrn2014, aes(x= attitude, y=points, col=gender))
B <- A +geom_point()
C <- B + geom_smooth(method = "lm")
Figure1 <- C + ggtitle("Figure 1. The relationship between attitude and points")
Figure1
D <- ggplot(lrn2014, aes(x=stra, y=points, col=gender))
D1 <- D + geom_point()
D2 <- D1 + geom_smooth(method="lm")
Figure2 <- D2 + ggtitle("Figure 2. The relationship between strategic learning and points")
Figure2
library(GGally)
ggpairs(lrn2014, lower= list(combo = wrap("facethist", bins = 20)))
my_reg <- lm(points ~ attitude + stra + surf, data = lrn2014 )
summary(my_reg)
my_reg2 <- lm(points ~ attitude + stra, data = lrn2014)
summary(my_reg2)
my_reg4 <- lm(points ~ attitude + gender, data=lrn2014)
summary(my_reg4)
errors <- plot(my_reg2, which= c(1,2,5), par(mfrow= c(2,2)))
errors2 <- plot(my_reg3, which= c(1,2,5), par(mfrow= c(2,2)))
errors2 <- plot(my_reg3, which= c(1,2,5), par(mfrow= c(2,2)))
