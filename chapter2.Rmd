# Regression analysis - why attitude matters

"*This week I have completed DataCamp exercises, peer reviewed Rstudio excersice 1 and surprisingly enough learned for the first time to create regression models in the RStudio excercise 2. This has been an excellent opportunity to learn and interpret regression model many-sidedly and thoroughly.*" - January 2017


##Part 1 
is the data wrangling exercise, which doesn't show on my course diary. If you are interested, go and check in my github: https://github.com/macabset/IODS-project/blob/master/data/create_learning2014.R


```{r}

getwd()

lrn2014 <- read.csv("learning2014.csv")

head(lrn2014)
```

##Part 2: Analysis (max 15p)
###2.1: Exploring dataset

```{r}
str(lrn2014)
dim(lrn2014)
```

Here we can see the learning2014 dataset, whih has 7 variables with 166 observations. 
  This questionnary conserning students's approaches to learning has been collected from a course:" Introduction to Social Statistics, fall 2014" (Finnish).
  Dataset has been modified so that questions have been set as combined variables (gender, age, attitude, deep, stra, surf and points) and 0 values in variable points have been removed.

###2.2 Describing dataset

####Summary

```{r}
summary(lrn2014)
library(ggplot2)
```

In our summary we can see that all variables except gender are numeric with discrete values.

#### Picturing our dataset

```{r}
A <- ggplot(lrn2014, aes(x= attitude, y=points, col=gender))
B <- A +geom_point()
C <- B + geom_smooth(method = "lm")
Figure1 <- C + ggtitle("Figure 1. The relationship between attitude and points")
Figure1
```

Figure 1 shows us how attitude towards statistics correlate to the points of the exam. In this case, such as in many others, motivation partly explains success.


####EXTRA: Combinig strategic learning to points.
```{r}
D <- ggplot(lrn2014, aes(x=stra, y=points, col=gender))
D1 <- D + geom_point()
D2 <- D1 + geom_smooth(method="lm")
Figure2 <- D2 + ggtitle("Figure 2. The relationship between strategic learning and points")
Figure2
```

If we look at Figure 2 there seems to be a very small linear relationship between strategic learning and exam points. If student used strategic learning method in general, he or she might have gained a bit better exam points.

###2.3 Regression model
####Choosing explaining variables

```{r}
library(GGally)
ggpairs(lrn2014, lower= list(combo = wrap("facethist", bins = 20)))
```

As we can see in our plot matrix, variables attitude (0.437), stra (0.146) and surf (0.144) correlate the most with the points. Lets try those as explanatory variables in our regression model.

####Explaining points with attitude, strategic learning  and surface learning

*Regression model 1*
```{r}
my_reg <- lm(points ~ attitude + stra + surf, data = lrn2014 )
summary(my_reg)
```

Attitude seems to be the only one statistically significant in explaining points. This result is compatitive with our result in Figure 2, where correlation between strategic learning and points was bearly noticeable.

Let's remove first surf and see again our model

*Regression model 2*
```{r}
my_reg2 <- lm(points ~ attitude + stra, data = lrn2014)
summary(my_reg2)
```

The strategic learning gives still too high p-value, so we cannot exclude it being significant by chance.

Let's try our regression model with only attitude explaining it.

*Regression model 3*
```{r}
my_reg3 <- lm(points ~ attitude, data=lrn2014)
summary(my_reg3)
```

It seems that attitude towards statistics is our key explanatory variable to exam points. The more positive attitude toward statistics in general the better outcome in exam.

However, we should consider our model as a whole, so let's check first our R-squared values.



####How about gender?
What I didn't try last time was the significance of gender. I'm not sure how it reacts, since it is not continuous variable...
*Regression model 4*
```{r}
my_reg4 <- lm(points ~ attitude + gender, data=lrn2014)
summary(my_reg4)
```

According to results it would seem that it's almost 50/50 chance that the negative correlation between points and gender is there only by chance. If it were more significant, it would mean that women score better in test. 

Problems with gender as a factor in regression  model
* the gender-variable isn't a scale: "being less man gets you better points" isn't very reasonable answer
*how would only gender explain anything? It would be significant, if we were to compare what is the attitude between genders and perhaps reasons behind differences (if there are any)

For statisticians it might be obvious that gender doesn't work as explanatory variable, but testing with the model itself and interpreting the results might open the problems more clearly than just stating the fact. And it's fun to try!


### 2.4 Interpreting R-squared

R-squared in our regression model tells us how well our model fits to observations. To simplify, the bigger the R-squared the better our model seems to fit to observations. 

Our 3 tests of regression model show that our Multiple R-squared lowers each time we drop a variable. That happens, because it increases every time you add a variable, so drop in itself isn't a bad thing.  What we also see is that the 2nd test has the highest *adjusted R-square*. This should mean that even though we have more variables than in the 3rd one, it seems to be a better model in quality, and not just better fit because we add or lose variables.

I conclude that we still should consider bringing back the strategic learning to our multiple regression model, even though its p-value is over magical line of 0.05. Now we can check if regression model 2 is really viable by checking the residuals in our model.

###2.5 Errors in our model

Let's check Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage with our regression model 2.

```{r}
errors <- plot(my_reg2, which= c(1,2,5), par(mfrow= c(2,2)))
```

In plot 1 we see, that values are reaasonably well scattered through out the area, which means that errors make no pattern, as is expected from a good model.

In plot 2 we see, that our observations lines up to a line, so the errors seem to be normally distributed. This is consistent with our assumption, atlhough our model is less suitable in the both ends of values.
  
In plot 3 we see, that no single observation has too much weight on the model, leverage line stayes under 0.05 in the whole plot.

#### Comparing with model 3
```{r}
errors2 <- plot(my_reg3, which= c(1,2,5), par(mfrow= c(2,2)))
```

The error models are quite similar with model 2. The biggest difference is in the plot 3: REsiduals vs Leverage. Points are packed in the left end, but all the values are still under 0.05. It means that no single observation wights too much, that is, is not too different from other values to give false idea of the model.

Nevertheless, we concluded in the R-squared comparison that the model 2 is better than model 3, so let's stick to that, although these errors alone would not support that assumption.

###2.6 To conclude

our regression model 2, with explanatory variables attitude and strategic learning, exam points are explained with linear model. Considering the r-squared and checking with residuals, it indeed seems that the 2nd regression model best explains points. That means the more positive attitude towards statistics and the more strategically you wish to learn, the better outcome you get from statistics exam.


