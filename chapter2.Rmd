# Regression analysis - why attitude matters

*This week I have completed DataCamp exercises, peer reviewed Rstudio excersice 1 and surprisingly enough learned for the first time to create regression models in the RStudio excercise 2. This has been an excellent opportunity to learn to interpret regression model many-sidedly and thoroughly.*

##Part 1 
is the data wrangling exercise, which doesn't show on my course diary. If you are interested, go and check in my github: https://github.com/macabset/IODS-project/blob/master/data/create_learning2014.R


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE) 
full_learn14<- read.table( "http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
library(dplyr)

deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D07","D14","D22","D30")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_columns <- select(full_learn14, one_of(deep_questions))
full_learn14$deep <- rowMeans(deep_columns)
surface_columns <- select(full_learn14, one_of(surface_questions))
full_learn14$surf <- rowMeans(surface_columns)
strategic_columns <- select(full_learn14, one_of(strategic_questions))
full_learn14$stra <- rowMeans(strategic_columns)
New_columns <- c("gender","Age","Attitude", "deep", "stra", "surf", "Points")
sub_learn14 <- select(full_learn14, one_of(New_columns))
colnames(sub_learn14)[2] <- "age"
colnames(sub_learn14)[3] <- "attitude"
colnames(sub_learn14)[7]<- "points"
colnames(sub_learn14)
str(sub_learn14)

sub_learn14 <- filter(sub_learn14, points >! 0)


write.csv(sub_learn14, file = "C:/Users/Murmeli/Documents/GitHub/IODS-project/data/learning2014.csv", row.names = FALSE)
lrn2014 <- read.csv("C:/Users/Murmeli/Documents/GitHub/IODS-project/data/learning2014.csv")

head(lrn2014)
```

##Part 2: Analysis (max 15p)
###2.1: Exploring our dataset

```{r}
str(lrn2014)
dim(lrn2014)
```

Here we can see the learning2014 dataset, whih has 7 variables with 166 observations. 
  This questionnary conserning students's approaches to learning has been collected from a course:" Introduction to Social Statistics, fall 2014" (Finnish).
  Dataset has been modified so that questions have been combined to combined variables and 0 values in variable points have been removed.

```{r}
summary(lrn2014)
library(ggplot2)
```

In our summary we can see that all variables except gender are numeric with discrete values.

###2.2 Picturing our dataset

```{r}
A <- ggplot(lrn2014, aes(x= attitude, y=points, col=gender))
B <- A +geom_point()
C <- B + geom_smooth(method = "lm")
Figure1 <- C + ggtitle("Figure 1. The relationship between attitude and points")
Figure1
```

Figure 1 shows us how attitude towards statistics correlate to the points of the exam. In this case, such as in many others, motivation explains partly success.


###EXTRA: Combinig strategic learning to points.
```{r}
D <- ggplot(lrn2014, aes(x=stra, y=points, col=gender))
D1 <- D + geom_point()
D2 <- D1 + geom_smooth(method="lm")
Figure2 <- D2 + ggtitle("Figure 2. The relationship between strategic learning and points")
Figure2
```

If we look at Figure 2 there seems to be a very small linear relationship between strategic learning and exam points. If student used strategic learning method in general, he or she might have gained a bit better exam points.

###2.3 Regression model
*Choosing explaining variables*

```{r}
library(GGally)
ggpairs(lrn2014, lower= list(combo = wrap("facethist", bins = 20)))
```

As we can see in our plot matrix, variables attitude, stra and surf correlate the most with the points. Lets try those as explanatory variables in our regression model.

*Explaining points with attitude, strategic learning  and surface learning*

Regression model 1
```{r}
my_reg <- lm(points ~ attitude + stra + surf, data = lrn2014 )
summary(my_reg)
```

Attitude seems to be the only one statistically significant in explaining points. This result is compatitive with our result in Figure 2, where correlation between strategic learning and points was bearly noticeable.

Let's remove first surf and see again our model

Regression model 2
```{r}
my_reg2 <- lm(points ~ attitude + stra, data = lrn2014)
summary(my_reg2)
```

The strategic learning gives still too high p-value, so we cannot exclude it being significant by chance.

Let's try our regression model with only attitude explaining it.

Regression model 3
```{r}
my_reg3 <- lm(points ~ attitude, data=lrn2014)
summary(my_reg3)
```

It seems that attitude towards statistics is our key explanatory variable to exam points. The more positive attitude toward statistics in general the better outcome in exam.

However, we should consider our model as a whole, so let's check first our R-squared values.

### 2.4 Interpreting R-squared

R-squared in our regression model tells us how well our model fits to observations. To simplify, the bigger the R-squared the better our model seems to fit to observations. 

Our 3 tests of regression model show that our Multiple R-squared lowers each time we drop a variable. That happens, because it increases every time you add a variable, so drop in itself isn't a bad thing.  What we also see is that the 2nd test has the highest *adjusted R-square*. This should mean that even though we have more variables than in the 3rd one, it seems to be a better model in quality, and not just better fit because we add or lose variables.

I conclude that we still should consider bringing back the strategic learning to our multiple regression model, even though its p-value is over magical line of 0.05. Now we can check if regression model 2 is really viable by checking the residuals in our model.

###2.5 Errors in our model

Let's check Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage with our regression model 2.

```{r}
errors <- plot(my_reg2, which= c(1,2,5), par(mfrow= c(2,2)))
```

In plot 1 we see, that values are reaasonably well scattered through out the area, which means that errors make no pattern, as is expected from a good model.

In plot 2 we see, that our observations lines up to a line, so the errors seem to be normally distributed. This is consistent with our assumption, atlhough our model is less suitable in the both ends of values.
  
In plot 3 we see, that no single observation has too much weight on the model, leverage line stayes under 0.05 in the whole plot.

##Part 3: To conclude

our regression model 2, with explanatory variables attitude and strategic learning, exam points are explained with linear model. Considering the r-squared and checking with residuals, it indeed seems that the 2nd regression model best explains points. That means the more positive attitude towards statistics and the more strategically you wish to learn, the better outcome you get from statistics exam.


